{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hperrin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\hperrin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# # Perso\n",
    "sys.path.append('../Algo')\n",
    "from NeuralNetworks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff(t_a, t_b):\n",
    "    t_diff = relativedelta(t_a, t_b)\n",
    "    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = make_scorer(score_func = log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = pd.read_csv(\"../w79/numerai_datasets/numerai_training_data.csv\")\n",
    "Xtest = pd.read_csv(\"../w79/numerai_datasets/numerai_tournament_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = pd.concat([Xtrain, Xtest[Xtest['data_type'] == 'validation']], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((609578, 54), (348831, 54))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_data = Xtest\n",
    "ids = Xtest['id']\n",
    "\n",
    "Xtest = Xtrain[Xtrain['data_type'] == 'validation']\n",
    "Xtrain = Xtrain[Xtrain['data_type'] != 'validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((535713, 50), (535713,), (73865, 50), (73865,), (348831, 50))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain = Xtrain['target']\n",
    "Ytest = Xtest['target']\n",
    "\n",
    "Xtrain.drop(['id', 'era', 'data_type', 'target'], inplace = True, axis = 1)\n",
    "Xtest.drop(['id', 'era', 'data_type', 'target'], inplace = True, axis = 1)\n",
    "real_data.drop(['id', 'era', 'data_type', 'target'], inplace = True, axis = 1)\n",
    "\n",
    "Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape, real_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining models and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cores = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = ['ExtraTrees', \n",
    "               'XGBoost', \n",
    "               'SGDC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                               criterion = 'entropy', \n",
    "                               max_depth = 4, \n",
    "                               n_estimators = 100, \n",
    "                               bootstrap = True),\n",
    "          \n",
    "          XGBClassifier(learning_rate = 0.5, \n",
    "                        max_depth = 3, \n",
    "                        n_estimators = 75,\n",
    "                        nthread = n_cores),\n",
    "          \n",
    "          SGDClassifier(loss = 'log', \n",
    "                        penalty = 'elasticnet', \n",
    "                        learning_rate = 'optimal',\n",
    "                        n_jobs = n_cores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_to_tune = [{'min_samples_split' : [200,1000],                            # ExtraTreesClassifier\n",
    "                       'min_samples_leaf' : [200,1000]},\n",
    "                    \n",
    "                      {'subsample' : [0.5, 0.75, 1]},                                   # XGBoostClassifier\n",
    "\n",
    "                      {'alpha' : [0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1],            # SGDClassifier\n",
    "                       'l1_ratio' : [0.001, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning parameters and getting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging_steps = 10\n",
    "n_feature = 20\n",
    "\n",
    "features = [name for name in Xtrain.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_stage_train = pd.DataFrame()\n",
    "first_stage_test = pd.DataFrame()\n",
    "first_stage_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees\n",
      "\n",
      "Step 1...done in 0h 1m 37s\n",
      "log loss : 0.6928227973886574\n",
      "\n",
      "Step 2...done in 0h 1m 37s\n",
      "log loss : 0.6927354370871803\n",
      "\n",
      "Step 3...done in 0h 1m 41s\n",
      "log loss : 0.6926697392843302\n",
      "\n",
      "Step 4...done in 0h 1m 44s\n",
      "log loss : 0.6928330244160793\n",
      "\n",
      "Step 5...done in 0h 1m 48s\n",
      "log loss : 0.6927236465158352\n",
      "\n",
      "Step 6...done in 0h 1m 46s\n",
      "log loss : 0.6927539761659607\n",
      "\n",
      "Step 7...done in 0h 2m 2s\n",
      "log loss : 0.6928067547989717\n",
      "\n",
      "Step 8...done in 0h 1m 46s\n",
      "log loss : 0.6926585238728056\n",
      "\n",
      "Step 9...done in 0h 1m 37s\n",
      "log loss : 0.6927144925483121\n",
      "\n",
      "Step 10...done in 0h 1m 33s\n",
      "log loss : 0.6927338631086211\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing XGBoost\n",
      "\n",
      "Step 1...done in 0h 4m 12s\n",
      "log loss : 0.6936589846549777\n",
      "\n",
      "Step 2...done in 0h 4m 33s\n",
      "log loss : 0.6940892488871884\n",
      "\n",
      "Step 3...done in 0h 4m 50s\n",
      "log loss : 0.693791661117142\n",
      "\n",
      "Step 4...done in 0h 3m 55s\n",
      "log loss : 0.6937455820688694\n",
      "\n",
      "Step 5...done in 0h 4m 16s\n",
      "log loss : 0.693720921029785\n",
      "\n",
      "Step 6...done in 0h 4m 12s\n",
      "log loss : 0.6942635373996832\n",
      "\n",
      "Step 7...done in 0h 4m 6s\n",
      "log loss : 0.6937111931101596\n",
      "\n",
      "Step 8...done in 0h 4m 8s\n",
      "log loss : 0.694275252244661\n",
      "\n",
      "Step 9...done in 0h 4m 8s\n",
      "log loss : 0.6938572136046383\n",
      "\n",
      "Step 10...done in 0h 4m 9s\n",
      "log loss : 0.6938446989461627\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing SGDC\n",
      "\n",
      "Step 1...done in 0h 1m 39s\n",
      "log loss : 0.6931710289348297\n",
      "\n",
      "Step 2...done in 0h 1m 37s\n",
      "log loss : 0.6931474808555667\n",
      "\n",
      "Step 3...done in 0h 1m 37s\n",
      "log loss : 0.6931472326487723\n",
      "\n",
      "Step 4...done in 0h 1m 37s\n",
      "log loss : 0.6931485423667145\n",
      "\n",
      "Step 5...done in 0h 1m 37s\n",
      "log loss : 0.6931471784527521\n",
      "\n",
      "Step 6...done in 0h 1m 37s\n",
      "log loss : 0.6931472592792531\n",
      "\n",
      "Step 7...done in 0h 1m 37s\n",
      "log loss : 0.6931482546888788\n",
      "\n",
      "Step 8...done in 0h 1m 37s\n",
      "log loss : 0.6931472958096411\n",
      "\n",
      "Step 9...done in 0h 1m 36s\n",
      "log loss : 0.6931471828529245\n",
      "\n",
      "Step 10...done in 0h 1m 37s\n",
      "log loss : 0.6931471839249852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, model, parameters in zip(model_names, models, parameters_to_tune):\n",
    "    \n",
    "    time1 = datetime.now()\n",
    "    print('\\n---------------------------------------------')\n",
    "    print('>> Processing {}\\n'.format(name))\n",
    "    \n",
    "    for step in range(bagging_steps):\n",
    "        \n",
    "        time2 = datetime.now()\n",
    "        print(\"Step {}\".format(step+1), end = '...')\n",
    "        \n",
    "        # Creating data\n",
    "        np.random.shuffle(features)\n",
    "        train = Xtrain[features[:n_feature]]\n",
    "        test = Xtest[features[:n_feature]]\n",
    "        final = real_data[features[:n_feature]]\n",
    "        \n",
    "        # Tuning\n",
    "        gscv = GridSearchCV(model, parameters, scoring = score, n_jobs = n_cores)\n",
    "        gscv.fit(train, Ytrain)\n",
    "\n",
    "        # Saving best predictions\n",
    "        first_stage_train['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(train)[:,1]\n",
    "        first_stage_test['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(test)[:,1]\n",
    "        first_stage_final['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(final)[:,1]\n",
    "\n",
    "        print('done in {}'.format(diff(datetime.now(),time2)))\n",
    "        print('log loss : {}\\n'.format(log_loss(Ytest, first_stage_test['{}_prediction_{}'.format(name,step+1)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_stage_train.to_csv('../w79/first_stage_train.csv')\n",
    "first_stage_test.to_csv('../w79/first_stage_test.csv')\n",
    "first_stage_final.to_csv('../w79/first_stage_final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_stage_train = pd.read_csv(\"../w79/first_stage_train.csv\")\n",
    "first_stage_test = pd.read_csv(\"../w79/first_stage_test.csv\")\n",
    "first_stage_final = pd.read_csv('../w79/first_stage_final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_stage_train.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "first_stage_test.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "first_stage_final.drop(['Unnamed: 0'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((535713, 30), (73865, 30), (348831, 30))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_stage_train.shape, first_stage_test.shape, first_stage_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There only are numerical features. Since tensorflow only accept numpy matrix we have to tranform our data. Furthermore we have to reformat the target shape and standardize our data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set : (535713, 30) (535713, 2)\n",
      "Final set : (348831, 30)\n",
      "Test set : (73865, 30) (73865, 2)\n"
     ]
    }
   ],
   "source": [
    "def reformat(labels, num_labels):\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "first_stage_train = first_stage_train.as_matrix().astype(np.float32)\n",
    "Ytrain = reformat(Ytrain, 2)\n",
    "\n",
    "first_stage_test = first_stage_test.as_matrix().astype(np.float32)\n",
    "Ytest = reformat(Ytest, 2)\n",
    "\n",
    "first_stage_final = first_stage_final.as_matrix().astype(np.float32)\n",
    "\n",
    "print('Training set :', first_stage_train.shape, Ytrain.shape)\n",
    "print('Final set :', first_stage_final.shape)\n",
    "print('Test set :', first_stage_test.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = StandardScaler().fit(first_stage_train)\n",
    "\n",
    "# Scale the train set\n",
    "first_stage_train = scaler.transform(first_stage_train)\n",
    "\n",
    "# Scale the test set\n",
    "first_stage_test = scaler.transform(first_stage_test)\n",
    "\n",
    "# Scale the final data\n",
    "first_stage_final = scaler.transform(first_stage_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINITION\n",
    "model = NeuralNetworkClassifier(layers = [20],\n",
    "                                num_steps = 150000,\n",
    "                                display_step = 5000,\n",
    "                                learning_rate = 0.001,\n",
    "                                L2Regression = 0.05,\n",
    "                                dropout = 0.2,\n",
    "                                learning_rate_decay = 0.9,\n",
    "                                batch_size = 500,\n",
    "                                verbose = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- PROCESSING LEARNING --------------------\n",
      "\n",
      "Step : 0   Minibatch loss : 2.357624053955078   Validation loss : 1.0758034455943422\n",
      "Step : 5000   Minibatch loss : 1.1798838376998901   Validation loss : 0.6905245721927364\n",
      "Step : 10000   Minibatch loss : 0.9808145761489868   Validation loss : 0.6906055666636814\n",
      "Step : 15000   Minibatch loss : 0.8695586323738098   Validation loss : 0.6907181092841693\n",
      "Step : 20000   Minibatch loss : 0.8008138537406921   Validation loss : 0.6907582694836306\n",
      "Step : 25000   Minibatch loss : 0.7517625689506531   Validation loss : 0.6906569838973363\n",
      "Step : 30000   Minibatch loss : 0.7272824645042419   Validation loss : 0.6905339117561108\n",
      "Step : 35000   Minibatch loss : 0.7146121859550476   Validation loss : 0.6903906456924687\n",
      "Step : 40000   Minibatch loss : 0.7037126421928406   Validation loss : 0.6902316691825545\n",
      "Step : 45000   Minibatch loss : 0.7003671526908875   Validation loss : 0.6901135874748848\n",
      "Step : 50000   Minibatch loss : 0.6991966962814331   Validation loss : 0.6899604949722484\n",
      "Step : 55000   Minibatch loss : 0.6949676275253296   Validation loss : 0.6898162637590204\n",
      "Step : 60000   Minibatch loss : 0.6904101967811584   Validation loss : 0.6896957558683816\n",
      "Step : 65000   Minibatch loss : 0.6915766596794128   Validation loss : 0.6895665721178987\n",
      "Step : 70000   Minibatch loss : 0.6902950406074524   Validation loss : 0.6894347268881534\n",
      "Step : 75000   Minibatch loss : 0.6862992644309998   Validation loss : 0.6893142333277635\n",
      "Step : 80000   Minibatch loss : 0.6949676871299744   Validation loss : 0.6892083512558239\n",
      "Step : 85000   Minibatch loss : 0.6933929324150085   Validation loss : 0.6891172922239573\n",
      "Step : 90000   Minibatch loss : 0.6954973340034485   Validation loss : 0.6890662221462053\n",
      "Step : 95000   Minibatch loss : 0.6928579807281494   Validation loss : 0.6890082566686592\n",
      "Step : 100000   Minibatch loss : 0.6948629021644592   Validation loss : 0.688927149135503\n",
      "Step : 105000   Minibatch loss : 0.688727617263794   Validation loss : 0.6888928539840212\n",
      "Step : 110000   Minibatch loss : 0.6915014982223511   Validation loss : 0.6888500921689044\n",
      "Step : 115000   Minibatch loss : 0.6879667639732361   Validation loss : 0.6888071232896336\n",
      "Step : 120000   Minibatch loss : 0.6928931474685669   Validation loss : 0.6887728889010325\n",
      "Step : 125000   Minibatch loss : 0.6977466344833374   Validation loss : 0.6887705915979553\n",
      "Step : 130000   Minibatch loss : 0.6947988867759705   Validation loss : 0.6887309091368732\n",
      "Step : 135000   Minibatch loss : 0.6891529560089111   Validation loss : 0.6887195602496207\n",
      "Step : 140000   Minibatch loss : 0.691814124584198   Validation loss : 0.6886999524613332\n",
      "Step : 145000   Minibatch loss : 0.6893995404243469   Validation loss : 0.688671480391806\n",
      "\n",
      ">> Test loss : 0.6921590676076409\n",
      "Optimization time : 0h 3m 17s\n"
     ]
    }
   ],
   "source": [
    "# model.fit(first_stage_train, Ytrain, first_stage_test, Ytest, validation=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((609578, 30), (609578, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.concatenate([first_stage_train,first_stage_test], axis = 0)\n",
    "training_label = np.concatenate([Ytrain, Ytest], axis = 0)\n",
    "\n",
    "training_data.shape, training_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- PROCESSING LEARNING --------------------\n",
      "\n",
      "Step : 0   Minibatch loss : 2.2011709213256836\n",
      "Step : 5000   Minibatch loss : 1.194794774055481\n",
      "Step : 10000   Minibatch loss : 0.9961936473846436\n",
      "Step : 15000   Minibatch loss : 0.8722558617591858\n",
      "Step : 20000   Minibatch loss : 0.804010808467865\n",
      "Step : 25000   Minibatch loss : 0.7625634074211121\n",
      "Step : 30000   Minibatch loss : 0.7293069362640381\n",
      "Step : 35000   Minibatch loss : 0.7159773707389832\n",
      "Step : 40000   Minibatch loss : 0.7078022956848145\n",
      "Step : 45000   Minibatch loss : 0.7028517723083496\n",
      "Step : 50000   Minibatch loss : 0.6976909637451172\n",
      "Step : 55000   Minibatch loss : 0.6936199069023132\n",
      "Step : 60000   Minibatch loss : 0.6927656531333923\n",
      "Step : 65000   Minibatch loss : 0.6912948489189148\n",
      "Step : 70000   Minibatch loss : 0.6908606290817261\n",
      "Step : 75000   Minibatch loss : 0.6940179467201233\n",
      "Step : 80000   Minibatch loss : 0.6925798654556274\n",
      "Step : 85000   Minibatch loss : 0.6908078789710999\n",
      "Step : 90000   Minibatch loss : 0.6961158514022827\n",
      "Step : 95000   Minibatch loss : 0.6920832991600037\n",
      "Step : 100000   Minibatch loss : 0.6939020752906799\n",
      "Step : 105000   Minibatch loss : 0.6899659633636475\n",
      "Step : 110000   Minibatch loss : 0.6922403573989868\n",
      "Step : 115000   Minibatch loss : 0.6926811933517456\n",
      "Step : 120000   Minibatch loss : 0.6891351938247681\n",
      "Step : 125000   Minibatch loss : 0.6947765350341797\n",
      "Step : 130000   Minibatch loss : 0.6941900253295898\n",
      "Step : 135000   Minibatch loss : 0.6984636187553406\n",
      "Step : 140000   Minibatch loss : 0.6925160884857178\n",
      "Step : 145000   Minibatch loss : 0.6882217526435852\n",
      "Optimization time : 0h 3m 19s\n"
     ]
    }
   ],
   "source": [
    "# REAL MODEL\n",
    "model = NeuralNetworkClassifier(layers = [20],\n",
    "                                num_steps = 150000,\n",
    "                                display_step = 5000,\n",
    "                                learning_rate = 0.001,\n",
    "                                L2Regression = 0.05,\n",
    "                                dropout = 0.2,\n",
    "                                learning_rate_decay = 0.9,\n",
    "                                batch_size = 500,\n",
    "                                verbose = None)\n",
    "\n",
    "model.fit(training_data,\n",
    "          training_label,\n",
    "          validation = None,\n",
    "          final_data = first_stage_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_prediction = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_submit = pd.DataFrame()\n",
    "nn_submit['id'] = ids\n",
    "nn_submit['probability'] = final_prediction\n",
    "\n",
    "nn_submit.to_csv('../w79/4th_submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Weighted Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create meta-features\n",
    "first_stage_train = pd.DataFrame(first_stage_train)\n",
    "first_stage_train['meta1'] = Xtrain.std(axis = 1)\n",
    "\n",
    "first_stage_test = pd.DataFrame(first_stage_test)\n",
    "first_stage_test['meta1'] = Xtest.std(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss : 0.70031755802259\n"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(first_stage_train, Ytrain)\n",
    "\n",
    "predicted = linear_model.predict(first_stage_test)\n",
    "\n",
    "print(\"Log loss : {}\".format(log_loss(Ytest, predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "104px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
