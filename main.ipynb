{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hperrin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\hperrin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Machine Learning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lightgbm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# # Perso\n",
    "sys.path.append('C:/Users/hperrin/Desktop/Numerai/Algo')\n",
    "from NeuralNetworks import *\n",
    "\n",
    "def diff(t_a, t_b):\n",
    "    t_diff = relativedelta(t_a, t_b)\n",
    "    return '{h}h {m}m {s}s'.format(h=t_diff.hours, m=t_diff.minutes, s=t_diff.seconds)\n",
    "\n",
    "score = make_scorer(score_func = log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(535713, 50) (535713,) (73865, 50) (73865,) (348689, 50)\n"
     ]
    }
   ],
   "source": [
    "Xtrain = pd.read_csv(\"C:/Users/hperrin/Desktop/Numerai/w80/numerai_datasets/numerai_training_data.csv\")\n",
    "Xtest = pd.read_csv(\"C:/Users/hperrin/Desktop/Numerai/w80/numerai_datasets/numerai_tournament_data.csv\")\n",
    "\n",
    "real_data = Xtest\n",
    "ids = Xtest['id']\n",
    "\n",
    "Xtest = Xtest[Xtest['data_type'] == 'validation']\n",
    "\n",
    "Ytrain = Xtrain['target']\n",
    "Ytest = Xtest['target']\n",
    "\n",
    "Xtrain.drop(['id', 'era', 'data_type', 'target'], inplace = True, axis = 1)\n",
    "Xtest.drop(['id', 'era', 'data_type', 'target'], inplace = True, axis = 1)\n",
    "real_data.drop(['id', 'era', 'data_type', 'target'], inplace = True, axis = 1)\n",
    "\n",
    "print(Xtrain.shape, Ytrain.shape, Xtest.shape, Ytest.shape, real_data.shape)\n",
    "\n",
    "metafeature_train = pd.DataFrame()\n",
    "metafeature_test = pd.DataFrame()\n",
    "metafeature_final = pd.DataFrame()\n",
    "\n",
    "# Variance\n",
    "metafeature_train['variance'] = Xtrain.std(axis = 1)\n",
    "metafeature_test['variance'] = Xtest.std(axis = 1)\n",
    "metafeature_final['variance'] = real_data.std(axis = 1)\n",
    "\n",
    "# Moyenne\n",
    "metafeature_train['mean'] = Xtrain.mean(axis = 1)\n",
    "metafeature_test['mean'] = Xtest.mean(axis = 1)\n",
    "metafeature_final['mean'] = real_data.mean(axis = 1)\n",
    "\n",
    "# Distance to mean individual\n",
    "mean_indiv = Xtrain.mean(axis = 0)\n",
    "\n",
    "metafeature_train['distance'] = Xtrain.apply(lambda row: euclidean(row,mean_indiv), axis = 1)\n",
    "metafeature_test['distance'] = Xtest.apply(lambda row: euclidean(row,mean_indiv), axis = 1)\n",
    "metafeature_final['distance'] = real_data.apply(lambda row: euclidean(row,mean_indiv), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cores = -1\n",
    "\n",
    "first_stage_train = pd.DataFrame()\n",
    "first_stage_test = pd.DataFrame()\n",
    "first_stage_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = ['ExtraTrees1',\n",
    "               'ExtraTrees2',\n",
    "               'ExtraTrees3',\n",
    "               'ExtraTrees4',\n",
    "               'ExtraTrees5',\n",
    "               'ExtraTrees6']\n",
    "\n",
    "\n",
    "models =[ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                              criterion = 'entropy', \n",
    "                              n_estimators = 100, \n",
    "                              bootstrap = True,\n",
    "                              min_samples_split = 10,\n",
    "                              max_depth = 3,\n",
    "                              min_samples_leaf = 10),\n",
    "         \n",
    "         ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                              criterion = 'gini', \n",
    "                              n_estimators = 100, \n",
    "                              bootstrap = True,\n",
    "                              min_samples_split = 10,\n",
    "                              max_depth = 3,\n",
    "                              min_samples_leaf = 10),\n",
    "         \n",
    "         ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                              criterion = 'entropy', \n",
    "                              n_estimators = 100, \n",
    "                              bootstrap = True,\n",
    "                              min_samples_split = 500,\n",
    "                              max_depth = 3,\n",
    "                              min_samples_leaf = 500),\n",
    "         \n",
    "         ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                              criterion = 'gini', \n",
    "                              n_estimators = 100, \n",
    "                              bootstrap = True,\n",
    "                              min_samples_split = 500,\n",
    "                              max_depth = 3,\n",
    "                              min_samples_leaf = 500),\n",
    "        \n",
    "         ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                              criterion = 'entropy', \n",
    "                              n_estimators = 100, \n",
    "                              bootstrap = True,\n",
    "                              min_samples_split = 2000,\n",
    "                              max_depth = 3,\n",
    "                              min_samples_leaf = 2000),\n",
    "         \n",
    "         ExtraTreesClassifier(n_jobs = n_cores, \n",
    "                              criterion = 'gini', \n",
    "                              n_estimators = 100, \n",
    "                              bootstrap = True,\n",
    "                              min_samples_split = 2000,\n",
    "                              max_depth = 3,\n",
    "                              min_samples_leaf = 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees1\n",
      "\n",
      "Step 1...done in 0h 0m 19s\n",
      "log loss : 0.6926960021613164\n",
      "\n",
      "Step 2...done in 0h 0m 19s\n",
      "log loss : 0.6927608024834063\n",
      "\n",
      "Step 3...done in 0h 0m 19s\n",
      "log loss : 0.6926353219442417\n",
      "\n",
      "Step 4...done in 0h 0m 19s\n",
      "log loss : 0.6927974337007831\n",
      "\n",
      "Step 5...done in 0h 0m 19s\n",
      "log loss : 0.692675707216713\n",
      "\n",
      "Step 6...done in 0h 0m 19s\n",
      "log loss : 0.6926875145936503\n",
      "\n",
      "Step 7...done in 0h 0m 19s\n",
      "log loss : 0.6927901372496886\n",
      "\n",
      "Step 8...done in 0h 0m 18s\n",
      "log loss : 0.6928138150983699\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees2\n",
      "\n",
      "Step 1...done in 0h 0m 19s\n",
      "log loss : 0.6927737572754838\n",
      "\n",
      "Step 2...done in 0h 0m 19s\n",
      "log loss : 0.6926904243359732\n",
      "\n",
      "Step 3...done in 0h 0m 19s\n",
      "log loss : 0.6926466453275585\n",
      "\n",
      "Step 4...done in 0h 0m 19s\n",
      "log loss : 0.6926649243774308\n",
      "\n",
      "Step 5...done in 0h 0m 19s\n",
      "log loss : 0.692673541160151\n",
      "\n",
      "Step 6...done in 0h 0m 19s\n",
      "log loss : 0.6927819494971073\n",
      "\n",
      "Step 7...done in 0h 0m 19s\n",
      "log loss : 0.6926281507581574\n",
      "\n",
      "Step 8...done in 0h 0m 19s\n",
      "log loss : 0.6927998150675231\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees3\n",
      "\n",
      "Step 1...done in 0h 0m 19s\n",
      "log loss : 0.6927766225770544\n",
      "\n",
      "Step 2...done in 0h 0m 19s\n",
      "log loss : 0.6927088385520588\n",
      "\n",
      "Step 3...done in 0h 0m 19s\n",
      "log loss : 0.6926672577081543\n",
      "\n",
      "Step 4...done in 0h 0m 19s\n",
      "log loss : 0.6926383168857962\n",
      "\n",
      "Step 5...done in 0h 0m 19s\n",
      "log loss : 0.6926562944222366\n",
      "\n",
      "Step 6...done in 0h 0m 19s\n",
      "log loss : 0.6925817074944274\n",
      "\n",
      "Step 7...done in 0h 0m 19s\n",
      "log loss : 0.6926504441878959\n",
      "\n",
      "Step 8...done in 0h 0m 19s\n",
      "log loss : 0.6928042479108352\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees4\n",
      "\n",
      "Step 1...done in 0h 0m 19s\n",
      "log loss : 0.692777507397152\n",
      "\n",
      "Step 2...done in 0h 0m 19s\n",
      "log loss : 0.6927608802950518\n",
      "\n",
      "Step 3...done in 0h 0m 19s\n",
      "log loss : 0.6927512760582204\n",
      "\n",
      "Step 4...done in 0h 0m 19s\n",
      "log loss : 0.6926260728245158\n",
      "\n",
      "Step 5...done in 0h 0m 19s\n",
      "log loss : 0.6926979261231803\n",
      "\n",
      "Step 6...done in 0h 0m 19s\n",
      "log loss : 0.6927161003092761\n",
      "\n",
      "Step 7...done in 0h 0m 20s\n",
      "log loss : 0.6926675240602398\n",
      "\n",
      "Step 8...done in 0h 0m 21s\n",
      "log loss : 0.6927825717434606\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees5\n",
      "\n",
      "Step 1...done in 0h 0m 20s\n",
      "log loss : 0.6927000773068165\n",
      "\n",
      "Step 2...done in 0h 0m 20s\n",
      "log loss : 0.6927724636282132\n",
      "\n",
      "Step 3...done in 0h 0m 19s\n",
      "log loss : 0.6928000244651235\n",
      "\n",
      "Step 4...done in 0h 0m 19s\n",
      "log loss : 0.6928148037153331\n",
      "\n",
      "Step 5...done in 0h 0m 19s\n",
      "log loss : 0.6927444808157398\n",
      "\n",
      "Step 6...done in 0h 0m 19s\n",
      "log loss : 0.6926464980051704\n",
      "\n",
      "Step 7...done in 0h 0m 19s\n",
      "log loss : 0.6928128056525089\n",
      "\n",
      "Step 8...done in 0h 0m 19s\n",
      "log loss : 0.692647901785674\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      ">> Processing ExtraTrees6\n",
      "\n",
      "Step 1...done in 0h 0m 18s\n",
      "log loss : 0.6927290054003875\n",
      "\n",
      "Step 2...done in 0h 0m 19s\n",
      "log loss : 0.6927079239104851\n",
      "\n",
      "Step 3...done in 0h 0m 19s\n",
      "log loss : 0.6927440482209004\n",
      "\n",
      "Step 4...done in 0h 0m 19s\n",
      "log loss : 0.6927683578827937\n",
      "\n",
      "Step 5...done in 0h 0m 19s\n",
      "log loss : 0.6927250296380936\n",
      "\n",
      "Step 6...done in 0h 0m 19s\n",
      "log loss : 0.6927267948282297\n",
      "\n",
      "Step 7...done in 0h 0m 19s\n",
      "log loss : 0.6927035934033136\n",
      "\n",
      "Step 8...done in 0h 0m 19s\n",
      "log loss : 0.6927360803789946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bagging_steps = 8\n",
    "n_feature = 20\n",
    "\n",
    "features = [name for name in Xtrain.columns]\n",
    "\n",
    "for name,model in zip(model_names, models):\n",
    "    \n",
    "    time1 = datetime.now()\n",
    "    print('\\n---------------------------------------------')\n",
    "    print('>> Processing {}\\n'.format(name))\n",
    "    \n",
    "    for step in range(bagging_steps):\n",
    "        \n",
    "        time2 = datetime.now()\n",
    "        print(\"Step {}\".format(step+1), end = '...')\n",
    "        \n",
    "        # Creating data\n",
    "        np.random.shuffle(features)\n",
    "        train = Xtrain[features[:n_feature]]\n",
    "        test = Xtest[features[:n_feature]]\n",
    "        final = real_data[features[:n_feature]]\n",
    "        \n",
    "        # Adding metafeature weight\n",
    "        for feature in train.columns:\n",
    "            for meta in ['variance', 'mean', 'distance']:\n",
    "                train['{}_{}'.format(feature, meta)] = train[feature] * metafeature_train['{}'.format(meta)]\n",
    "                test['{}_{}'.format(feature, meta)] = test[feature] * metafeature_test['{}'.format(meta)]\n",
    "                final['{}_{}'.format(feature, meta)] = final[feature] * metafeature_final['{}'.format(meta)]\n",
    "        \n",
    "        # Tuning\n",
    "        gscv = model\n",
    "        gscv.fit(train, Ytrain)\n",
    "\n",
    "        # Saving best predictions\n",
    "        first_stage_train['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(train)[:,1]\n",
    "        first_stage_test['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(test)[:,1]\n",
    "        first_stage_final['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(final)[:,1]\n",
    "\n",
    "        print('done in {}'.format(diff(datetime.now(),time2)))\n",
    "        print('log loss : {}\\n'.format(log_loss(Ytest, first_stage_test['{}_prediction_{}'.format(name,step+1)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_names = ['SGDClassifier']\n",
    "\n",
    "models = [SGDClassifier(loss = 'log', \n",
    "                        penalty = 'elasticnet', \n",
    "                        learning_rate = 'optimal',\n",
    "                        n_jobs = n_cores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------\n",
      ">> Processing SGDClassifier\n",
      "\n",
      "Step 1...done in 0h 0m 8s\n",
      "log loss : 0.6937208467964969\n",
      "\n",
      "Step 2...done in 0h 0m 8s\n",
      "log loss : 0.6964528878648224\n",
      "\n",
      "Step 3...done in 0h 0m 8s\n",
      "log loss : 0.6967101867526921\n",
      "\n",
      "Step 4...done in 0h 0m 8s\n",
      "log loss : 0.692345115600741\n",
      "\n",
      "Step 5...done in 0h 0m 8s\n",
      "log loss : 0.692742063962092\n",
      "\n",
      "Step 6...done in 0h 0m 8s\n",
      "log loss : 0.6956599242569506\n",
      "\n",
      "Step 7...done in 0h 0m 8s\n",
      "log loss : 0.6954561903829398\n",
      "\n",
      "Step 8...done in 0h 0m 9s\n",
      "log loss : 0.6962481959235597\n",
      "\n",
      "Step 9...done in 0h 0m 9s\n",
      "log loss : 0.6925834231379141\n",
      "\n",
      "Step 10...done in 0h 0m 10s\n",
      "log loss : 0.6925822783686636\n",
      "\n",
      "Step 11...done in 0h 0m 8s\n",
      "log loss : 0.6937710717176553\n",
      "\n",
      "Step 12...done in 0h 0m 8s\n",
      "log loss : 0.6932572174189645\n",
      "\n",
      "Step 13...done in 0h 0m 8s\n",
      "log loss : 0.6956014075355812\n",
      "\n",
      "Step 14...done in 0h 0m 8s\n",
      "log loss : 0.6927080504237146\n",
      "\n",
      "Step 15...done in 0h 0m 8s\n",
      "log loss : 0.692588559144487\n",
      "\n",
      "Step 16...done in 0h 0m 8s\n",
      "log loss : 0.6963068910008052\n",
      "\n",
      "Step 17...done in 0h 0m 8s\n",
      "log loss : 0.6933363457952841\n",
      "\n",
      "Step 18...done in 0h 0m 8s\n",
      "log loss : 0.6926714431459428\n",
      "\n",
      "Step 19...done in 0h 0m 8s\n",
      "log loss : 0.6925289351024217\n",
      "\n",
      "Step 20...done in 0h 0m 8s\n",
      "log loss : 0.6936268915959056\n",
      "\n",
      "Step 21...done in 0h 0m 8s\n",
      "log loss : 0.6920950452718975\n",
      "\n",
      "Step 22...done in 0h 0m 8s\n",
      "log loss : 0.6922011614239141\n",
      "\n",
      "Step 23...done in 0h 0m 8s\n",
      "log loss : 0.6934600905106026\n",
      "\n",
      "Step 24...done in 0h 0m 8s\n",
      "log loss : 0.6928263545352574\n",
      "\n",
      "Step 25...done in 0h 0m 8s\n",
      "log loss : 0.6925354130222072\n",
      "\n",
      "Step 26...done in 0h 0m 8s\n",
      "log loss : 0.6970482636156512\n",
      "\n",
      "Step 27...done in 0h 0m 8s\n",
      "log loss : 0.6946058085194973\n",
      "\n",
      "Step 28...done in 0h 0m 8s\n",
      "log loss : 0.6923917450410664\n",
      "\n",
      "Step 29...done in 0h 0m 8s\n",
      "log loss : 0.6932369955761883\n",
      "\n",
      "Step 30...done in 0h 0m 8s\n",
      "log loss : 0.6931218864079849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bagging_steps = 30\n",
    "n_feature = 15\n",
    "\n",
    "features = [name for name in Xtrain.columns]\n",
    "\n",
    "for name,model in zip(model_names, models):\n",
    "    \n",
    "    time1 = datetime.now()\n",
    "    print('\\n---------------------------------------------')\n",
    "    print('>> Processing {}\\n'.format(name))\n",
    "    \n",
    "    for step in range(bagging_steps):\n",
    "        \n",
    "        time2 = datetime.now()\n",
    "        print(\"Step {}\".format(step+1), end = '...')\n",
    "        \n",
    "        # Creating data\n",
    "        np.random.shuffle(features)\n",
    "        train = Xtrain[features[:n_feature]]\n",
    "        test = Xtest[features[:n_feature]]\n",
    "        final = real_data[features[:n_feature]]\n",
    "        \n",
    "        # Adding metafeature weight\n",
    "        for feature in train.columns:\n",
    "            for meta in ['variance', 'mean', 'distance']:\n",
    "                train['{}_{}'.format(feature, meta)] = train[feature] * metafeature_train['{}'.format(meta)]\n",
    "                test['{}_{}'.format(feature, meta)] = test[feature] * metafeature_test['{}'.format(meta)]\n",
    "                final['{}_{}'.format(feature, meta)] = final[feature] * metafeature_final['{}'.format(meta)]\n",
    "\n",
    "        # Tuning\n",
    "        gscv = model\n",
    "        gscv.fit(train, Ytrain)\n",
    "\n",
    "        # Saving best predictions\n",
    "        first_stage_train['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(train)[:,1]\n",
    "        first_stage_test['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(test)[:,1]\n",
    "        first_stage_final['{}_prediction_{}'.format(name, step+1)] = gscv.predict_proba(final)[:,1]\n",
    "\n",
    "        print('done in {}'.format(diff(datetime.now(),time2)))\n",
    "        print('log loss : {}\\n'.format(log_loss(Ytest, first_stage_test['{}_prediction_{}'.format(name,step+1)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(first_stage_train).to_csv('C:/Users/hperrin/Desktop/Numerai/w80/first_stage_train.csv', index = False)\n",
    "pd.DataFrame(first_stage_test).to_csv('C:/Users/hperrin/Desktop/Numerai/w80/first_stage_test.csv', index = False)\n",
    "pd.DataFrame(first_stage_final).to_csv('C:/Users/hperrin/Desktop/Numerai/w80/first_stage_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_stage_train = pd.read_csv('C:/Users/hperrin/Desktop/Numerai/w80/first_stage_train.csv')\n",
    "first_stage_test = pd.read_csv('C:/Users/hperrin/Desktop/Numerai/w80/first_stage_test.csv')\n",
    "first_stage_final = pd.read_csv('C:/Users/hperrin/Desktop/Numerai/w80/first_stage_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adding metafeature weight\n",
    "# for feature in first_stage_train.columns:\n",
    "#     for meta in ['variance', 'mean', 'distance']:\n",
    "#         first_stage_train['{}_{}'.format(feature, meta)] = first_stage_train[feature] * metafeature_train['{}'.format(meta)]\n",
    "#         first_stage_test['{}_{}'.format(feature, meta)] = first_stage_test[feature] * metafeature_test['{}'.format(meta)]\n",
    "#         first_stage_final['{}_{}'.format(feature, meta)] = first_stage_final[feature] * metafeature_final['{}'.format(meta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(labels, num_labels):\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "first_stage_train = first_stage_train.as_matrix().astype(np.float32)\n",
    "training_label = reformat(Ytrain, 2)\n",
    "\n",
    "first_stage_test = first_stage_test.as_matrix().astype(np.float32)\n",
    "testing_label = reformat(Ytest, 2)\n",
    "\n",
    "first_stage_final = first_stage_final.as_matrix().astype(np.float32)\n",
    "\n",
    "# Define the scaler\n",
    "scaler = StandardScaler().fit(first_stage_train)\n",
    "\n",
    "# Scale the train set\n",
    "first_stage_train = scaler.transform(first_stage_train)\n",
    "\n",
    "# Scale the test set\n",
    "first_stage_test = scaler.transform(first_stage_test)\n",
    "\n",
    "# Scale the final data\n",
    "first_stage_final = scaler.transform(first_stage_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- PROCESSING LEARNING --------------------\n",
      "\n",
      "Step : 0   Minibatch loss : 1.7021031379699707\n",
      "Step : 1000   Minibatch loss : 1.502763032913208\n",
      "Step : 2000   Minibatch loss : 1.3883020877838135\n",
      "Step : 3000   Minibatch loss : 1.2990739345550537\n",
      "Step : 4000   Minibatch loss : 1.230311393737793\n",
      "Step : 5000   Minibatch loss : 1.1590685844421387\n",
      "Step : 6000   Minibatch loss : 1.1208832263946533\n",
      "Step : 7000   Minibatch loss : 1.089595913887024\n",
      "Step : 8000   Minibatch loss : 1.0394099950790405\n",
      "Step : 9000   Minibatch loss : 1.0046164989471436\n",
      "Step : 10000   Minibatch loss : 0.9802024364471436\n",
      "Step : 11000   Minibatch loss : 0.9498657584190369\n",
      "Step : 12000   Minibatch loss : 0.9265963435173035\n",
      "Step : 13000   Minibatch loss : 0.9036453366279602\n",
      "Step : 14000   Minibatch loss : 0.8730961680412292\n",
      "Step : 15000   Minibatch loss : 0.8591055274009705\n",
      "Step : 16000   Minibatch loss : 0.8438348174095154\n",
      "Step : 17000   Minibatch loss : 0.8274432420730591\n",
      "Step : 18000   Minibatch loss : 0.817733645439148\n",
      "Step : 19000   Minibatch loss : 0.8077155947685242\n",
      "Optimization time : 0h 0m 52s\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetworkClassifier(layers = [20],\n",
    "                                activation = 'tanh',\n",
    "                                num_steps = 20000,\n",
    "                                display_step = 1000,\n",
    "                                learning_rate = 0.001,\n",
    "                                L2Regression = 0.05,\n",
    "                                dropout = 0.4,\n",
    "                                learning_rate_decay = 1,\n",
    "                                batch_size = 1000,\n",
    "                                verbose = None)\n",
    "\n",
    "# model.fit(first_stage_train,\n",
    "#           training_label,\n",
    "#           first_stage_test,\n",
    "#           testing_label,\n",
    "#           validation = 0.2)\n",
    "\n",
    "\n",
    "model.fit(first_stage_train,\n",
    "          training_label,\n",
    "          final_data = first_stage_final)\n",
    "\n",
    "NN_pred = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_stage_train = pd.DataFrame(first_stage_train)\n",
    "first_stage_test = pd.DataFrame(first_stage_test)\n",
    "first_stage_final = pd.DataFrame(first_stage_final)\n",
    "\n",
    "# Adding metafeature weight\n",
    "for feature in first_stage_train.columns:\n",
    "    for meta in ['variance', 'mean', 'distance']:\n",
    "        first_stage_train['{}_{}'.format(feature, meta)] = first_stage_train[feature] * metafeature_train['{}'.format(meta)]\n",
    "        first_stage_test['{}_{}'.format(feature, meta)] = first_stage_test[feature] * metafeature_test['{}'.format(meta)]\n",
    "        first_stage_final['{}_{}'.format(feature, meta)] = first_stage_final[feature] * metafeature_final['{}'.format(meta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss : 0.6924954783078869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=3, \n",
    "                    n_estimators=100, \n",
    "                    learning_rate=0.05).fit(first_stage_train, Ytrain)\n",
    "\n",
    "xgb_pred = xgb.predict_proba(first_stage_test)[:,1]\n",
    "\n",
    "print('log loss : {}\\n'.format(log_loss(Ytest, xgb_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict_proba(first_stage_final)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = ids\n",
    "submission['probability'] = (NN_pred + xgb_pred) / 2\n",
    "\n",
    "submission.to_csv('C:/Users/hperrin/Desktop/Numerai/w80/5th_submit.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
